from litellm import completion

def generate_response(query, context_chunks, api_key):
    # Define the maximum length for the context
    MAX_CONTEXT_LENGTH = 4096

    # Concatenate text from context chunks, limiting the total length
    context = "\n".join([chunk.get('text', '') for chunk in context_chunks[:5] if 'text' in chunk])[:MAX_CONTEXT_LENGTH]

    # Prepare the messages for the LLM model
    messages = [
        {"role": "system", "content": "You are an assistant that answers questions using the provided context."},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
    ]

    try:
        # Call the LLM completion API
        response = completion(model="groq/llama3-8b-8192", messages=messages, api_key=api_key)
        
        # Extract and return the content of the response if available
        if 'choices' in response and response['choices']:
            return response['choices'][0]['message']['content'].strip()
        else:
            return "No response generated by the model."
    except Exception as e:
        # Handle errors and return an error message
        return f"An error occurred while generating the response: {e}"